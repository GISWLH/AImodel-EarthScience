{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee1eefd1-2183-426b-9563-c7ce3ccba72d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pip install -i https://pypi.tuna.tsinghua.edu.cn/simple opencv-python\n",
    "import os\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import utils\n",
    "import os\n",
    "os.environ['KERAS_BACKEND']='tensorflow'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b42e0e5-2518-4d26-8ba6-cd864ac0aff3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../DLdataset/UCMerced_LandUse/Images/forest\n",
      "../DLdataset/UCMerced_LandUse/Images/buildings\n",
      "../DLdataset/UCMerced_LandUse/Images/river\n",
      "../DLdataset/UCMerced_LandUse/Images/mobilehomepark\n",
      "../DLdataset/UCMerced_LandUse/Images/harbor\n",
      "../DLdataset/UCMerced_LandUse/Images/golfcourse\n",
      "../DLdataset/UCMerced_LandUse/Images/agricultural\n",
      "../DLdataset/UCMerced_LandUse/Images/runway\n",
      "../DLdataset/UCMerced_LandUse/Images/baseballdiamond\n",
      "../DLdataset/UCMerced_LandUse/Images/overpass\n",
      "../DLdataset/UCMerced_LandUse/Images/chaparral\n",
      "../DLdataset/UCMerced_LandUse/Images/tenniscourt\n",
      "../DLdataset/UCMerced_LandUse/Images/intersection\n",
      "../DLdataset/UCMerced_LandUse/Images/airplane\n",
      "../DLdataset/UCMerced_LandUse/Images/parkinglot\n",
      "../DLdataset/UCMerced_LandUse/Images/sparseresidential\n",
      "../DLdataset/UCMerced_LandUse/Images/mediumresidential\n",
      "../DLdataset/UCMerced_LandUse/Images/denseresidential\n",
      "../DLdataset/UCMerced_LandUse/Images/beach\n",
      "../DLdataset/UCMerced_LandUse/Images/freeway\n",
      "../DLdataset/UCMerced_LandUse/Images/storagetanks\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "labelnames = ['forest', 'buildings', 'river', 'mobilehomepark', 'harbor', 'golfcourse', 'agricultural', 'runway', 'baseballdiamond', 'overpass', 'chaparral', 'tenniscourt', 'intersection', 'airplane', 'parkinglot', 'sparseresidential', 'mediumresidential', 'denseresidential', 'beach', 'freeway', 'storagetanks']\n",
    "labels =[]\n",
    "classes = 21\n",
    "cur_path = os.getcwd()\n",
    "root_folder = '../DLdataset/UCMerced_LandUse/Images/'\n",
    "\n",
    "\n",
    "for i in range(classes):\n",
    "    path = os.path.join(root_folder,labelnames[i])\n",
    "    print(path)\n",
    "    images = os.listdir(path)\n",
    "    \n",
    "\n",
    "    for a in images:\n",
    "        \n",
    "        try:\n",
    "            image = Image.open(path + '//'+ a)\n",
    "            image = image.resize((64,64))\n",
    "            image = np.array(image)\n",
    "            data.append(image)\n",
    "            labels.append(i)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "data = np.array(data)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Normalize the pixel values\n",
    "data = data.astype('float32')/255.0\n",
    "\n",
    "# One-hot encoding of labels\n",
    "# from keras.utils import np_utils\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# from keras.utils.np_utils import to_categorical\n",
    "\n",
    "labels = to_categorical(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dcb0b501-1e23-452e-b755-3610c1e2f649",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b40f6943-e106-4e85-b94c-c98a83bb51bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (2, 2), input_shape=(64, 64, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, (2, 2)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(21))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91926b28-a43f-4701-ab7a-3c608c71e7c7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "34/34 [==============================] - 2s 43ms/step - loss: 3.0295 - accuracy: 0.0708 - val_loss: 2.9484 - val_accuracy: 0.0786\n",
      "Epoch 2/40\n",
      "34/34 [==============================] - 1s 39ms/step - loss: 2.9060 - accuracy: 0.0839 - val_loss: 2.7927 - val_accuracy: 0.1095\n",
      "Epoch 3/40\n",
      "34/34 [==============================] - 1s 41ms/step - loss: 2.8394 - accuracy: 0.1113 - val_loss: 2.7393 - val_accuracy: 0.1381\n",
      "Epoch 4/40\n",
      "34/34 [==============================] - 1s 39ms/step - loss: 2.7428 - accuracy: 0.1149 - val_loss: 2.5847 - val_accuracy: 0.1548\n",
      "Epoch 5/40\n",
      "34/34 [==============================] - 1s 38ms/step - loss: 2.6643 - accuracy: 0.1411 - val_loss: 2.5583 - val_accuracy: 0.2214\n",
      "Epoch 6/40\n",
      "34/34 [==============================] - 1s 39ms/step - loss: 2.5832 - accuracy: 0.1762 - val_loss: 2.4282 - val_accuracy: 0.2333\n",
      "Epoch 7/40\n",
      "34/34 [==============================] - 1s 38ms/step - loss: 2.4976 - accuracy: 0.1863 - val_loss: 2.3252 - val_accuracy: 0.2738\n",
      "Epoch 8/40\n",
      "34/34 [==============================] - 1s 38ms/step - loss: 2.4083 - accuracy: 0.2185 - val_loss: 2.2531 - val_accuracy: 0.3286\n",
      "Epoch 9/40\n",
      "34/34 [==============================] - 1s 39ms/step - loss: 2.3577 - accuracy: 0.2244 - val_loss: 2.1917 - val_accuracy: 0.3476\n",
      "Epoch 10/40\n",
      "34/34 [==============================] - 1s 39ms/step - loss: 2.3135 - accuracy: 0.2512 - val_loss: 2.1524 - val_accuracy: 0.3690\n",
      "Epoch 11/40\n",
      "34/34 [==============================] - 1s 39ms/step - loss: 2.2015 - accuracy: 0.2690 - val_loss: 2.0417 - val_accuracy: 0.3881\n",
      "Epoch 12/40\n",
      "34/34 [==============================] - 1s 40ms/step - loss: 2.1507 - accuracy: 0.2863 - val_loss: 2.0064 - val_accuracy: 0.4357\n",
      "Epoch 13/40\n",
      "34/34 [==============================] - 1s 38ms/step - loss: 2.0992 - accuracy: 0.3030 - val_loss: 1.9594 - val_accuracy: 0.4119\n",
      "Epoch 14/40\n",
      "34/34 [==============================] - 1s 38ms/step - loss: 2.0553 - accuracy: 0.3315 - val_loss: 1.9229 - val_accuracy: 0.4524\n",
      "Epoch 15/40\n",
      "34/34 [==============================] - 1s 38ms/step - loss: 2.0116 - accuracy: 0.3250 - val_loss: 1.8501 - val_accuracy: 0.4762\n",
      "Epoch 16/40\n",
      "34/34 [==============================] - 1s 39ms/step - loss: 1.9897 - accuracy: 0.3167 - val_loss: 1.7872 - val_accuracy: 0.4667\n",
      "Epoch 17/40\n",
      "34/34 [==============================] - 1s 39ms/step - loss: 1.9378 - accuracy: 0.3345 - val_loss: 1.8340 - val_accuracy: 0.4810\n",
      "Epoch 18/40\n",
      "34/34 [==============================] - 1s 40ms/step - loss: 1.8945 - accuracy: 0.3548 - val_loss: 1.7790 - val_accuracy: 0.4714\n",
      "Epoch 19/40\n",
      "34/34 [==============================] - 1s 39ms/step - loss: 1.8921 - accuracy: 0.3530 - val_loss: 1.7723 - val_accuracy: 0.4667\n",
      "Epoch 20/40\n",
      "34/34 [==============================] - 1s 39ms/step - loss: 1.8848 - accuracy: 0.3625 - val_loss: 1.7498 - val_accuracy: 0.4810\n",
      "Epoch 21/40\n",
      "34/34 [==============================] - 1s 40ms/step - loss: 1.8301 - accuracy: 0.3679 - val_loss: 1.7054 - val_accuracy: 0.4810\n",
      "Epoch 22/40\n",
      "34/34 [==============================] - 1s 39ms/step - loss: 1.7124 - accuracy: 0.4077 - val_loss: 1.6637 - val_accuracy: 0.4976\n",
      "Epoch 23/40\n",
      "34/34 [==============================] - 1s 39ms/step - loss: 1.7340 - accuracy: 0.3893 - val_loss: 1.7160 - val_accuracy: 0.4786\n",
      "Epoch 24/40\n",
      "34/34 [==============================] - 1s 39ms/step - loss: 1.7377 - accuracy: 0.4107 - val_loss: 1.6944 - val_accuracy: 0.4833\n",
      "Epoch 25/40\n",
      "34/34 [==============================] - 1s 40ms/step - loss: 1.6941 - accuracy: 0.4238 - val_loss: 1.6762 - val_accuracy: 0.5048\n",
      "Epoch 26/40\n",
      "34/34 [==============================] - 1s 39ms/step - loss: 1.6847 - accuracy: 0.4214 - val_loss: 1.6429 - val_accuracy: 0.5048\n",
      "Epoch 27/40\n",
      "34/34 [==============================] - 1s 40ms/step - loss: 1.6554 - accuracy: 0.4345 - val_loss: 1.6252 - val_accuracy: 0.4786\n",
      "Epoch 28/40\n",
      "34/34 [==============================] - 1s 39ms/step - loss: 1.6439 - accuracy: 0.4268 - val_loss: 1.6558 - val_accuracy: 0.5071\n",
      "Epoch 29/40\n",
      "34/34 [==============================] - 1s 39ms/step - loss: 1.5762 - accuracy: 0.4577 - val_loss: 1.6540 - val_accuracy: 0.4952\n",
      "Epoch 30/40\n",
      "34/34 [==============================] - 1s 39ms/step - loss: 1.5992 - accuracy: 0.4417 - val_loss: 1.6098 - val_accuracy: 0.4976\n",
      "Epoch 31/40\n",
      "34/34 [==============================] - 1s 39ms/step - loss: 1.6275 - accuracy: 0.4351 - val_loss: 1.6139 - val_accuracy: 0.5190\n",
      "Epoch 32/40\n",
      "34/34 [==============================] - 1s 39ms/step - loss: 1.5749 - accuracy: 0.4381 - val_loss: 1.6870 - val_accuracy: 0.4952\n",
      "Epoch 33/40\n",
      "34/34 [==============================] - 1s 38ms/step - loss: 1.5887 - accuracy: 0.4250 - val_loss: 1.6054 - val_accuracy: 0.5190\n",
      "Epoch 34/40\n",
      "34/34 [==============================] - 1s 39ms/step - loss: 1.5036 - accuracy: 0.4756 - val_loss: 1.6307 - val_accuracy: 0.5119\n",
      "Epoch 35/40\n",
      "34/34 [==============================] - 1s 38ms/step - loss: 1.4828 - accuracy: 0.4780 - val_loss: 1.7724 - val_accuracy: 0.4905\n",
      "Epoch 36/40\n",
      "34/34 [==============================] - 1s 38ms/step - loss: 1.5676 - accuracy: 0.4464 - val_loss: 1.5626 - val_accuracy: 0.5238\n",
      "Epoch 37/40\n",
      "34/34 [==============================] - 1s 38ms/step - loss: 1.4712 - accuracy: 0.4851 - val_loss: 1.5792 - val_accuracy: 0.5381\n",
      "Epoch 38/40\n",
      "34/34 [==============================] - 1s 38ms/step - loss: 1.4385 - accuracy: 0.4768 - val_loss: 1.5930 - val_accuracy: 0.5143\n",
      "Epoch 39/40\n",
      "34/34 [==============================] - 1s 38ms/step - loss: 1.4827 - accuracy: 0.4613 - val_loss: 1.5849 - val_accuracy: 0.5310\n",
      "Epoch 40/40\n",
      "34/34 [==============================] - 1s 38ms/step - loss: 1.4305 - accuracy: 0.4839 - val_loss: 1.5771 - val_accuracy: 0.5238\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2a2e52b2a30>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=40, validation_data=(X_test, y_test), batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90a27a14-3767-47af-a275-0420eebdf4c3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: river\n",
      "Class probabilities: [[3.1773135e-01 1.5065636e-04 3.7655079e-01 1.0566279e-04 2.0423558e-04\n",
      "  7.9523362e-02 3.1503249e-02 8.2666622e-03 2.5942147e-02 8.6408021e-04\n",
      "  3.0674925e-02 8.4902108e-02 6.3060001e-03 7.3793263e-04 3.1532688e-04\n",
      "  2.1639206e-02 1.6753243e-03 2.2148907e-03 3.9221440e-04 5.2915984e-03\n",
      "  5.0082458e-03]]\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess a new image for prediction\n",
    "def preprocess_image(image_path):\n",
    "    image = Image.open(image_path)\n",
    "    image = image.resize((64, 64))\n",
    "    image = np.array(image)\n",
    "    image = image.astype('float32') / 255.0\n",
    "    image = np.expand_dims(image, axis=0)  # Add batch dimension\n",
    "    return image\n",
    "\n",
    "new_image_path = '../DLdataset/UCMerced_LandUse/Images/river/river04.tif'\n",
    "new_image = preprocess_image(new_image_path)\n",
    "\n",
    "# Predict the class of the new image\n",
    "predicted_probs = model.predict(new_image)\n",
    "predicted_class = np.argmax(predicted_probs)\n",
    "\n",
    "# Get the corresponding label name\n",
    "predicted_label = labelnames[predicted_class]\n",
    "\n",
    "print(f\"Predicted class: {predicted_label}\")\n",
    "print(\"Class probabilities:\", predicted_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bf8c1a-fa2b-4ed4-8156-095968e829c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
